[TOC]

结合 SLAM 问题的数学建模与相机成像过程，假设在 $\boldsymbol{x}_k$ 处对路标 $\boldsymbol{y}_j$ 进行了一次观测，对应到图像上的像素位置 $\boldsymbol{z}_{k,j}$ ，则观测方程为：
$$
s\boldsymbol{z}_{k,j}=\boldsymbol{K}(\boldsymbol{R}_k\boldsymbol{y}_j+\boldsymbol{t}_k)
$$
其中，$\boldsymbol{K}$ 为相机内参，$s$ 为像素点距离，也是相机坐标下的第三个分量，若使用变换矩阵 $\boldsymbol{T}_k$ 来描述，则需要进行齐次坐标的转换。

考虑噪声的影响，假设两个噪声项 $\boldsymbol{w}_k,\boldsymbol{v}_{k,j}$ 满足零均值的高斯分布：
$$
\boldsymbol{w}_k\sim\mathcal{N}(\bold{0},\boldsymbol{R}_k),\quad \boldsymbol{v}_{k,j}\sim\mathcal{N}(\bold{0},\boldsymbol{Q}_{k,j})
$$
则构成了一个**状态估计问题**：通过带噪声的数据 $\boldsymbol{z},\boldsymbol{u}$ 推断位姿 $\boldsymbol{x}$ 和地图 $\boldsymbol{y}$ 以及它们的概率分布

* 增量/渐进 incremental 方法，滤波器：关心当前时刻的状态估计 $\boldsymbol{x}_k$ ，对之前的状态不多做考虑
* 批量 batch 方法：可以在更大的范围内达到最优化
* 滑动窗口估计法：两者折衷的方法，固定一些历史轨迹，仅对当前时刻附近的一些轨迹进行优化

# 批量状态估计

考虑时刻 $1\sim N$ ，假设有 $M$ 个路标点，则定义所有时刻的机器人**位姿**和**路标点坐标**为：
$$
\boldsymbol{x}=\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\},\quad \boldsymbol{y}=\{\boldsymbol{y}_1,\cdots,\boldsymbol{y}_M\}
$$
$\boldsymbol{u}$ 表示所有时刻的**输入**，$\boldsymbol{z}$ 表示所有时刻的**观测数据**

则，对机器人状态的估计，从概率学角度来看，就是已知输入数据 $\boldsymbol{u}$ 和观测数据 $\boldsymbol{z}$ 的条件下，求状态 $\boldsymbol{x},\boldsymbol{y}$ 的条件概率分布
$$
P(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{z},\boldsymbol{u})
$$
特别地，当不知道控制输入只有图像时，即只考虑**观测方程**带来的数据时，相当于估计 $P(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{z})$ 的条件概率分布，这个问题也就是SfM

# 最大后验估计

使用**贝叶斯法则**：
$$
P(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{z},\boldsymbol{u})=\frac{P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})P(\boldsymbol{x},\boldsymbol{y})}{P(\boldsymbol{z},\boldsymbol{u})}\propto P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})P(\boldsymbol{x},\boldsymbol{y})
$$
式子左侧称为**后验概率**，右侧的 $P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})$ 称为**似然**，另一部分 $P(\boldsymbol{x},\boldsymbol{y})$ 称为**先验**。

直接求后验分布是困难的，但是求一个**状态最优估计**，使得在该状态下**后验概率最大化**是可行的：
$$
(\boldsymbol{x},\boldsymbol{y})^*_{\text{MAP}}=\arg\max P(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{z},\boldsymbol{u})=\arg\max P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})P(\boldsymbol{x},\boldsymbol{y})
$$
由贝叶斯法则可知：求解最大后验概率等价于**最大化似然和先验的乘积**

若没有先验，则求解**最大似然估计** Maximize Likelihood Estimation, MLE ：在什么样的状态下，最可能产生现在观测到的数据
$$
(\boldsymbol{x},\boldsymbol{y})^*_{\text{MLE}}=\arg\max P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})
$$

# 最小二乘

对某观测：$\boldsymbol{z}_{k,j}=h(\boldsymbol{y}_j,\boldsymbol{x}_k)+\boldsymbol{v}_{k,j}$ ，假设噪声项 $\boldsymbol{v}_{k,j}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{Q}_{k,j})$ ，则观测数据的条件概率为
$$
P(\boldsymbol{z}_{k,j}|\boldsymbol{x}_k,\boldsymbol{y}_j)=\mathcal{N}(h(\boldsymbol{y}_j,\boldsymbol{x}_k),\boldsymbol{Q}_{k,j})
$$
是一个高斯分布，通过**最小化负对数**求高斯分布最大似然，考虑高斯分布 $\boldsymbol{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ ，其概率密度函数与负对数为
$$
P(\boldsymbol{x})=\frac{1}{\sqrt{(2\pi)^N\det(\boldsymbol{\Sigma})}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)\\
-\ln(P(\boldsymbol{x}))=\frac{1}{2}\ln\left((2\pi)^N\det(\boldsymbol{\Sigma})\right)+\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})
$$
最小化右侧二次型项就可得到对状态的最大似然估计，代入 SLAM 观测模型
$$
\begin{split}
(\boldsymbol{x}_k,\boldsymbol{y}_j)^*&=\arg\max\mathcal{N}(h(\boldsymbol{y}_j,\boldsymbol{x}_k),\boldsymbol{Q}_{k,j})\\
&=\arg\min\left((\boldsymbol{z}_{k,j}-h(\boldsymbol{x}_k,\boldsymbol{y}_j))^T\boldsymbol{Q}_{k,j}^{-1}(\boldsymbol{z}_{k,j}-h(\boldsymbol{x}_k,\boldsymbol{y}_j))\right)
\end{split}
$$
该式等价于最小化**噪声项**（误差）的一个二次型，也即**马氏距离** Mahalanobis Distance，可以看作由信息矩阵 $\boldsymbol{Q}_{k,j}^{-1}$ （高斯分布协方差矩阵之逆）加权后的欧式距离

考虑批量时刻的数据，由于输入独立、观测独立、输入与观测之间独立，所以可以对联合分布进行因式分解：
$$
P(\boldsymbol{z},\boldsymbol{u}|\boldsymbol{x},\boldsymbol{y})=\prod_k{P(\boldsymbol{u}_k|\boldsymbol{x}_{k-1},\boldsymbol{x}_k)}\prod_{k,j}{P(\boldsymbol{z}_{k,j}|\boldsymbol{x}_k,\boldsymbol{y}_j)}
$$
由此说明可以独立地处理各时刻的运动和观测，定义各次输入和观测数据与模型之间的误差
$$
\boldsymbol{e}_{\boldsymbol{u},k}=\boldsymbol{x}_k-f(\boldsymbol{x}_{k-1},\boldsymbol{u}_k)\\
\boldsymbol{e}_{\boldsymbol{z},j,k}=\boldsymbol{z}_{k,j}-h(\boldsymbol{x}_k,\boldsymbol{y}_j)
$$
则，最小化**所有时刻估计值与真实读数之间的马氏距离**，等价于求**最大似然估计**
$$
\min J(\boldsymbol{x},\boldsymbol{y})=\sum_k{\boldsymbol{e}_{\boldsymbol{u},k}^T\boldsymbol{R}_k^{-1}\boldsymbol{e}_{\boldsymbol{u},k}}+\sum_{k,j}{\boldsymbol{e}_{\boldsymbol{z},k,j}^T\boldsymbol{R}_{k,j}^{-1}\boldsymbol{e}_{\boldsymbol{z},k,j}}
$$
这是一个**最小二乘问题**，它的解等价于状态的最大似然估计，

* 目标函数由多个误差的加权二次型组成，虽然总体状态变量维度很高但每个误差项很简单，仅与一两个状态变量有关，这使得整个问题呈现**稀疏**形式
* 若使用李代数表示增量，则该问题是**无约束**的最小二乘问题，但如果使用变换矩阵则会引入矩阵自身的约束
* 误差的分布会影响到这一项在整个问题中的权重，协方差矩阵越小，信息矩阵越大，所占权重越大

# 非线性最小二乘

整体思想：对最小二乘问题 $\min_{\boldsymbol{x}}F(\boldsymbol{x})=\frac{1}{2}\|f(\boldsymbol{x})\|_2^2$ 

1. 给定某个初始值 $\boldsymbol{x}_0$ 
2. 对于第 $k$ 次迭代，寻找一个增量 $\Delta\boldsymbol{x}_k$ ，使得 $\|f(\boldsymbol{x}+\Delta\boldsymbol{x})\|_2^2$ 处达到极小值
3. 若 $\Delta\boldsymbol{x}_k$ 足够小，则停止
4. 否则，令 $\boldsymbol{x}_{k+1}=\boldsymbol{x}_k+\Delta\boldsymbol{x}_k$ ，返回第二步

## 梯度法

考虑第 $k$ 次迭代，假设在 $\boldsymbol{x}_k$ 处，想要寻到增量 $\Delta\boldsymbol{x}_k$ ，则最直观的方式是将目标在 $\boldsymbol{x}_k$ 附近进行泰勒展开：
$$
F(\boldsymbol{x}_k+\Delta\boldsymbol{x}_k)\approx F(\boldsymbol{x}_k)+\boldsymbol{J}(\boldsymbol{x}_k)^T\Delta\boldsymbol{x}_k+\frac{1}{2}\Delta\boldsymbol{x}_k^T\boldsymbol{H}(x)_k\Delta\boldsymbol{x}_k
$$
其中，$\boldsymbol{J}(\boldsymbol{x})$ 是 $F(\boldsymbol{x})$ 关于 $\boldsymbol{x}$ 的一阶导数（也称为**梯度**、**雅可比 Jacobian 矩阵**），$\boldsymbol{H}$ 是二阶导数（**海塞 Hessian 矩阵**）

* **最速下降法**：保留一阶梯度信息，取增量为**反向的梯度**，只要沿着反向梯度方向前进，在一阶（线性）的近似下，目标函数必定会下降
* **牛顿法**：保留二阶梯度信息，求解线性方程 $\boldsymbol{H}\Delta\boldsymbol{x}=-\boldsymbol{J}$ 

## 高斯牛顿法

将 $f(x)$ 进行一阶泰勒展开
$$
f(\boldsymbol{x}+\Delta\boldsymbol{x})\approx f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})^T\Delta\boldsymbol{x}
$$
目标：寻找增量 $\Delta\boldsymbol{x}$ 使得 $\|f(\boldsymbol{x}+\Delta\boldsymbol{x})\|_2^2$ 达到最小，需要解一个线性最小二乘问题
$$
\Delta\boldsymbol{x}^*=\arg\min_{\Delta\boldsymbol{x}}\frac{1}{2}\|f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})^T\Delta\boldsymbol{x}\|^2
$$
将目标函数对 $\Delta\boldsymbol{x}$ 求导，并令导数为零可得到方程组：
$$
\boldsymbol{J}(\boldsymbol{x})\boldsymbol{J}^T(\boldsymbol{x})\Delta\boldsymbol{x}=-\boldsymbol{J}(\boldsymbol{x})f(\boldsymbol{x})
$$
此即**增量方程**，又称**高斯牛顿方程组**或**正规方程**，定义 $\boldsymbol{H}=\boldsymbol{J}(\boldsymbol{x})\boldsymbol{J}^T(\boldsymbol{x})\quad \boldsymbol{g}=-\boldsymbol{J}(\boldsymbol{x})f(\boldsymbol{x})$ ，则
$$
\boldsymbol{H}\Delta\boldsymbol{x}=\boldsymbol{g}
$$

* 对比牛顿法，高斯牛顿法使用 $\boldsymbol{J}\boldsymbol{J}^T$ 作为牛顿法中二阶海塞矩阵的近似，从而省略了计算 $\boldsymbol{H}$ 的过程

**求解增量方程**是整个优化问题的核心，高斯牛顿法的算法步骤为：

1. 给定某个初始值 $\boldsymbol{x}_0$ 
2. 对于第 $k$ 次迭代，求出当前的雅可比矩阵 $\boldsymbol{J}(\boldsymbol{x}_k)$ 和误差 $f(\boldsymbol{x}_k)$ 
3. 求解增量方程 $\boldsymbol{H}\Delta\boldsymbol{x}_k=\boldsymbol{g}$
4. 若 $\Delta\boldsymbol{x}_k$ 足够小，则停止；否则，令 $\boldsymbol{x}_{k+1}=\boldsymbol{x}_k+\Delta\boldsymbol{x}_k$ ，返回第二步

## 列文伯格——马夸尔特方法（阻尼牛顿法）

**信赖区域**：定义了在什么情况下二阶近似是有效的

定义指标 $\rho$ 来刻画近似的好坏程度，意义是实际函数下降的值与近似模型下降的值的比值：
$$
\rho=\frac{f(\boldsymbol{x}+\Delta\boldsymbol{x})-f(\boldsymbol{x})}{\boldsymbol{J}(\boldsymbol{x})^T\Delta\boldsymbol{x}}
$$
改良的非线性优化框架

1. 给定初始值 $\boldsymbol{x}_0$ ，以及优化半径 $\mu$ 

2. 对于第 $k$ 次迭代，在高斯牛顿法基础上加上信赖区域，求解：
   $$
   \min_{\Delta\boldsymbol{x}}\frac{1}{2}\|f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})^T\Delta\boldsymbol{x}\|^2,\quad \text{s.t.}\quad \|\boldsymbol{D}\Delta\boldsymbol{x}_k\|^2\le\mu
   $$
   其中，$\mu$ 是信赖区域的半径，$\boldsymbol{D}$ 为系数矩阵

3. 计算 $\rho$ 

4. 若 $\rho>\frac{3}{4}$ ，则设置 $\mu=2\mu$ 

5. 若 $\rho<\frac{1}{4}$ ，则设置 $\mu=0.5\mu$ 

6. 若 $\mu$ 大于某阈值，则认为近似可行，令 $\boldsymbol{x}_{k+1}=\boldsymbol{x}_k+\Delta\boldsymbol{x}_k$ 

7. 判断算法是否收敛，若不收敛则返回第二步，否则结束

求解步骤2中带不等约束的优化问题，用拉格朗日乘子把约束放到目标函数中去，构成拉格朗日函数：
$$
\mathcal{L}(\Delta\boldsymbol{x}_k,\lambda)=\frac{1}{2}\|f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})^T\Delta\boldsymbol{x}\|^2+\frac{\lambda}{2}\left(\|\boldsymbol{D}\Delta\boldsymbol{x}_k\|^2-\mu\right)
$$
这里的 $\lambda$ 为拉格朗日乘子，类似高斯牛顿法中的做法，令拉格朗日函数关于 $\Delta\boldsymbol{x}$ 的导数为零，它的核心仍是计算增量的线性方程：
$$
(\boldsymbol{H}+\lambda\boldsymbol{D}^T\boldsymbol{D})\Delta\boldsymbol{x}_k=\boldsymbol{g}
$$
相比高斯牛顿法，增量方程多了一项 $\lambda\boldsymbol{D}^T\boldsymbol{D}$ 

* 当参数 $\lambda$ 比较小时，$\boldsymbol{H}$ 占主要地位，说明二次近似模型在该范围内是比较好的，L-M 方法更接近于高斯牛顿法
* 当参数 $\lambda$ 比较大时，$\lambda\boldsymbol{D}^T\boldsymbol{D}$ 占主要地位，说明附近的二次近似不够好，L-M 方法更接近于一阶梯度下降法